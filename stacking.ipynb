{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以python自带的鸢尾花数据集为例\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "#切分一部分数据作为测试集\n",
    "x_train,x_test,y_train,y_text = train_test_split(X, y, test_size=0.2, random_state=914)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(object):\n",
    "    \"\"\"Parent class of basic models\"\"\"\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"return a trained model and eval metric o validation data\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, model, x_test):\n",
    "        \"\"\"return the predicted result\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_oof(self, x_train, y_train, x_test, n_folds = 5):\n",
    "        \"\"\"K-fold stacking\"\"\"\n",
    "        num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "        oof_train = np.zeros((num_train,3)) \n",
    "        oof_test = []\n",
    "        oof_test_all_fold = np.zeros((num_test, n_folds))\n",
    "        aucs = []\n",
    "        KF = KFold(n_splits = n_folds, random_state=2017)\n",
    "        for i, (train_index, val_index) in enumerate(KF.split(x_train)):\n",
    "            print('{0} fold, train {1}, val {2}'.format(i, len(train_index), len(val_index)))\n",
    "            x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "            x_val, y_val = x_train[val_index], y_train[val_index]\n",
    "            model, auc = self.train(x_tra, y_tra, x_val, y_val)\n",
    "            oof_train[val_index] = self.predict(model, x_val)\n",
    "            oof_test.append(self.predict(model, x_test))\n",
    "        oof_test = np.mean(np.hstack(oof_test), axis=1)\n",
    "#         print('all aucs {0}, average {1}'.format(aucs, np.mean(aucs)))\n",
    "        return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "class LGBClassifier(BasicModel):\n",
    "    def __init__(self):\n",
    "        self.num_boost_round = 2000\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "#             'objective': 'binary',\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_error',\n",
    "            'num_leaves': 80,\n",
    "            'learning_rate': 0.05,\n",
    "            # 'scale_pos_weight': 1.5,\n",
    "            'feature_fraction': 0.5,\n",
    "            'bagging_fraction': 1,\n",
    "            'bagging_freq': 5,\n",
    "            'max_bin': 300,\n",
    "            'is_unbalance': True,\n",
    "            'lambda_l2': 5.0,\n",
    "            'verbose' : -1,\n",
    "            'num_class':3\n",
    "            }\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print('train with lgb model')\n",
    "        lgbtrain = lgb.Dataset(x_train, y_train)\n",
    "        lgbval = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(self.params, \n",
    "                          lgbtrain,\n",
    "                          valid_sets = lgbval,\n",
    "                          verbose_eval = self.num_boost_round,\n",
    "                          num_boost_round = self.num_boost_round,\n",
    "                          early_stopping_rounds = self.early_stopping_rounds)\n",
    "        return model, model.best_score['valid_0']\n",
    "    \n",
    "    def predict(self, model, x_test):\n",
    "        print('test with lgb model')\n",
    "        a=model.predict(x_test, num_iteration=model.best_iteration)\n",
    "#         print(a.shape)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's multi_error: 0.416667\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "1 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's multi_error: 0.458333\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "2 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's multi_error: 0.583333\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "3 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's multi_error: 0.541667\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "4 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's multi_error: 0.5\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "(120, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "# get output of first layer models and construct as input for the second layer          \n",
    "lgb_classifier = LGBClassifier()\n",
    "lgb_oof_train, lgb_oof_test = lgb_classifier.get_oof(x_train, y_train, x_test)\n",
    "print(lgb_oof_train.shape, lgb_oof_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two models for first-layer stacking: xgb and lgb\n",
    "import xgboost as xgb\n",
    "class XGBClassifier(BasicModel):\n",
    "    def __init__(self):\n",
    "        \"\"\"set parameters\"\"\"\n",
    "        self.num_rounds=1000\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.params = {\n",
    "#             'objective': 'binary:logistic',\n",
    "            'objective': 'multi:softprob',\n",
    "            'eta': 0.1,\n",
    "            'max_depth': 8,\n",
    "#             'eval_metric': 'auc',\n",
    "            'metric': 'multi_error',\n",
    "            'seed': 0,\n",
    "            'silent' : 1,\n",
    "            'num_class':3\n",
    "            \n",
    "         }\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print('train with xgb model')\n",
    "        xgbtrain = xgb.DMatrix(x_train, y_train)\n",
    "        xgbval = xgb.DMatrix(x_val, y_val)\n",
    "        watchlist = [(xgbtrain,'train'), (xgbval, 'val')]\n",
    "        model = xgb.train(self.params, \n",
    "                          xgbtrain, \n",
    "                          self.num_rounds,\n",
    "                          watchlist,\n",
    "                          early_stopping_rounds = self.early_stopping_rounds)\n",
    "        return model, float(model.eval(xgbval).split()[1].split(':')[1])\n",
    "\n",
    "    def predict(self, model, x_test):\n",
    "        print('test with xgb model')\n",
    "        xgbtest = xgb.DMatrix(x_test)\n",
    "        return model.predict(xgbtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[2]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[3]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[4]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[5]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[6]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[7]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[8]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[9]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[10]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[11]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[12]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[13]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[14]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[15]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "1 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "[2]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[3]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[4]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[5]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[6]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[7]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[8]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[9]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[10]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[11]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[12]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[13]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[14]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[15]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "2 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[2]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[3]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[4]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[5]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[6]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[7]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[8]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[9]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[10]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[11]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[12]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[13]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[14]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[15]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "3 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:20] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "[2]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "[3]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "[4]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[5]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[6]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[7]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[8]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[9]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[10]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[11]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[12]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[13]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[14]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[15]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[16]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "4 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:20] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[2]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[3]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[4]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[5]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[6]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[7]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[8]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[9]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[10]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[11]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[12]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[13]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[14]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[15]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "(120, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier()\n",
    "xgb_oof_train, xgb_oof_test = xgb_classifier.get_oof(x_train, y_train, x_test)\n",
    "print(xgb_oof_train.shape, xgb_oof_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train = [xgb_oof_train, lgb_oof_train] \n",
    "input_test = np.array([xgb_oof_test, lgb_oof_test]).T\n",
    "input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 6) (30, 2)\n"
     ]
    }
   ],
   "source": [
    "stacked_train = np.concatenate(input_train, axis=1)\n",
    "stacked_test = input_test\n",
    "\n",
    "print(stacked_train.shape, stacked_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LR as the model of the second layer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for validation\n",
    "n = int(stacked_train.shape[0] * 0.8)\n",
    "x_tra, y_tra = stacked_train[:n], y_train[:n]\n",
    "x_val, y_val = stacked_train[n:], y_train[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出标签\n",
    "clf = svm.SVC(decision_function_shape='ovo',probability=False)\n",
    "clf.fit(x_tra,y_tra)\n",
    "y_pred=clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.01554226, 0.53502404, 0.44943371],\n",
       "       [0.01126673, 0.91413412, 0.07459915],\n",
       "       [0.01131326, 0.91407162, 0.07461512],\n",
       "       [0.01191187, 0.09403685, 0.89405128],\n",
       "       [0.01170821, 0.09416351, 0.89412828],\n",
       "       [0.93953054, 0.03317928, 0.02729018],\n",
       "       [0.93906664, 0.03347105, 0.02746231],\n",
       "       [0.01124835, 0.91409274, 0.07465891],\n",
       "       [0.93953054, 0.03317928, 0.02729018],\n",
       "       [0.01170821, 0.09416351, 0.89412828],\n",
       "       [0.01191187, 0.09403685, 0.89405128],\n",
       "       [0.01084207, 0.9026358 , 0.08652213],\n",
       "       [0.01176141, 0.094129  , 0.89410959],\n",
       "       [0.93953054, 0.03317928, 0.02729018],\n",
       "       [0.01176141, 0.094129  , 0.89410959],\n",
       "       [0.93906664, 0.03347105, 0.02746231],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.01091375, 0.90341611, 0.08567014],\n",
       "       [0.0180422 , 0.71640103, 0.26555677],\n",
       "       [0.01176141, 0.094129  , 0.89410959]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出概率\n",
    "clf = svm.SVC(decision_function_shape='ovo',probability=True)\n",
    "clf.fit(x_tra,y_tra)\n",
    "y_pred=clf.predict_proba(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1,\n",
       "        1, 2]),\n",
       " array([0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1,\n",
       "        2, 2]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8., 0., 0.],\n",
       "       [0., 8., 0.],\n",
       "       [0., 1., 7.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = compute_sample_weight(class_weight='balanced',y=y_val)\n",
    "cm =confusion_matrix(y_val, y_pred, sample_weight=sw)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 并没有用逻辑回归做鸢尾花的多分类\n",
    "# 逻辑回归\n",
    "model = LinearRegression()\n",
    "model.fit(x_tra,y_tra)\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "print(metrics.roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking:\n",
    "\n",
    "1. stacking的原理介绍：https://wulc.me/2018/01/21/stacking%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/\n",
    "2. stacking项目：https://github.com/WuLC/MachineLearningAlgorithm/blob/master/python/Stacking.py\n",
    "\n",
    "svc:https://blog.csdn.net/BabyBirdToFly/article/details/72886879\n",
    "\n",
    "xgb,lgb：https://www.cnblogs.com/nxf-rabbit75/p/9748345.html\n",
    "\n",
    "xgb，lgb参数介绍：https://www.jianshu.com/p/1100e333fcab\n",
    "\n",
    "逻辑回归多分类：https://blog.csdn.net/weixin_39541558/article/details/80621692\n",
    "\n",
    "鸢尾花多分类：https://blog.csdn.net/golden1314521/article/details/46564227\n",
    "\n",
    "macro,micro评价指标：https://zhuanlan.zhihu.com/p/59862986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
