{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归问题的加权融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归问题的加权融合是基于模型结果的，就是各个模型分别在全部数据集上进行训练和验证，得到最优模型，对测试集进行预测，然后，对各个模型测试集的预测值分配不同的权重，比较好的模型给与较大的权重，不好的模型你给予较小的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例：下列是3个模型的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred1 MAE: 0.1750000000000001\n",
      "Pred2 MAE: 0.07499999999999993\n",
      "Pred3 MAE: 0.10000000000000009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "#生成一些简单的样本数据， test_prei代表第i个模型的预测值\n",
    "test_pre1 = [1.2, 3.2, 2.1, 6.2]\n",
    "test_pre2 = [0.9, 3.1, 2.0, 5.9]\n",
    "test_pre3 = [1.1, 2.9, 2.2, 6.0]\n",
    "\n",
    "# y_test_true 代表模型的真实值\n",
    "y_test_true = [1, 3, 2, 6]\n",
    "\n",
    "# 可以先看一下各个模型的预测结果\n",
    "print('Pred1 MAE:',mean_absolute_error(y_test_true, test_pre1)) \n",
    "print('Pred2 MAE:',mean_absolute_error(y_test_true, test_pre2)) \n",
    "print('Pred3 MAE:',mean_absolute_error(y_test_true, test_pre3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted_pre MAE: 0.09528985507246435\n"
     ]
    }
   ],
   "source": [
    "# 加权融合\n",
    "def Weighted_method(test_pre1, test_pre2, test_pre3, w=[1/3, 1/3, 1/3]):\n",
    "    Weighted_result = w[0] * pd.Series(test_pre1) + w[1] * pd.Series(test_pre2) + w[2] * pd.Series(test_pre3)\n",
    "    return Weighted_result\n",
    "\n",
    "# 根据上面的MAE，我们计算每个模型的权重, 计算方式就是： wi = mae(i) / sum(mae)\n",
    "pred1_mae = 0.17\n",
    "pred2_mae = 0.075\n",
    "pred3_mae = 0.1\n",
    "ws = pred1_mae+pred2_mae+pred3_mae\n",
    "w = [pred1_mae/ws,pred2_mae/ws,pred3_mae/ws]\n",
    "\n",
    "Weighted_pre = Weighted_method(test_pre1,test_pre2,test_pre3,w) \n",
    "print('Weighted_pre MAE:',mean_absolute_error(y_test_true, Weighted_pre))   # 会发现这个效果会提高一些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了进行加权融合意外，还可以使用中位数和均值，这个均值是平均，而不是加权，而且这两种方法，效果还不怎么样，仅供参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义结果的mean平均函数\n",
    "def Mean_method(test_pre1,test_pre2,test_pre3):\n",
    "    Mean_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).mean(axis=1)\n",
    "    return Mean_result\n",
    "\n",
    "## 定义结果的中位数平均函数\n",
    "def Median_method(test_pre1,test_pre2,test_pre3):\n",
    "    Median_result = pd.concat([pd.Series(test_pre1),pd.Series(test_pre2),pd.Series(test_pre3)],axis=1).median(axis=1)\n",
    "    return Median_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类问题的投票融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn的投票法有2种类型，一种是直接输出类标签，另一种是输出类概率，使用前者进行投票叫做硬投票（Majority/Hard voting），使用后者进行投票叫做软投票（Soft Voting）。这个是通过VotingClassifier的voting参数进行控制的\n",
    "\n",
    "硬投票其实是少数服从多数的方式，但是，有时候少数服从多数并不行，因为模型的得分不同。那么，就需要更合理的投票方式，就是权值投票方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类问题，他们的结果可以输出概率，也可以输出标签。\n",
    "- 输出标签时，计算票数最多的类，该类为预测出来的类\n",
    "\n",
    "- 输出为概率时，有两种方法进行投票：\n",
    "    1. 比较每个模型中A和B的概率值，哪个大，就输出哪个类，比如有5个模型，有3个模型B类概率值大，那么 该样本预测值为B类【硬投票】\n",
    "    2. 概率即为权重，A类为每个模型的概率值和，B类为每个模型的概率值和，哪个类的概率值大，就用输出哪个类【软投票】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "硬投票是选择算法输出最多的标签，如果标签数量相等，那么按照升序进行选择，如下图所示\n",
    "![avatar](./image/hard_voting.jpg)\n",
    "\n",
    "硬投票对于某些模型对某类预测的概率严重高于某类，上图中，模型1和4对A的预测概率严重大于B，而其它三个模型的预测概率对B的预测概率只是勉强比A的概率值大，那么，使用硬投票方式就不太合理了，可以使用软投票"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "软投票使用各个算法输出的类概率进行类的选择，输入权重的话，会得到每个类的类概率的加权均值，值大的类会被选择\n",
    "![avatar](./image/soft_voting.jpg)\n",
    "\n",
    "使用soft voting时，把概率当作权重，这时候集成后的结果为A就显得更为合理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard voting投票"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.02) [XGBBoosting]\n",
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.96 (+/- 0.02) [Voting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris =load_iris()\n",
    "\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7,\n",
    "                     colsample_bytree=0.6, objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n",
    "                              min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1, probability=False)  # 软投票的时候，probability必须指定且为true\n",
    "\n",
    "# 硬投票\n",
    "eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='hard')\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Voting']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### soft voting软投票"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.02) [XGBBoosting]\n",
      "Accuracy: 0.33 (+/- 0.00) [Random Forest]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.96 (+/- 0.02) [Voting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris =load_iris()\n",
    "\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)\n",
    "\n",
    "clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7,\n",
    "                     colsample_bytree=0.6, objective='binary:logistic')\n",
    "clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4,\n",
    "                              min_samples_leaf=63,oob_score=True)\n",
    "clf3 = SVC(C=0.1, probability=True)  # 软投票的时候，probability必须指定且为true\n",
    "\n",
    "# 软投票\n",
    "eclf = VotingClassifier(estimators=[('xgb', clf1), ('rf', clf2), ('svc', clf3)], voting='soft')\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['XGBBoosting', 'Random Forest', 'SVM', 'Voting']):\n",
    "    scores = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型融合-stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking是一种融合方法，可以叠加很多层进行融合，下面以两层为例对stacking进行介绍。\n",
    "\n",
    "stacking对若干基模型进行交叉验证，将得到的预测结果进行拼接，作为次级模型的输入，同时，当每个基模型训练好后，需要对测试数据预测，使用训练好的次级模型对测试数据再进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示，存在3个基模型，对每个模型进行五折交叉验证，每一折中，对验证集和测试集同时预测，五折后，将基模型的验证集的预测值进行纵向拼接，得到训练集大小的预测值，该值作为会成为新特征；而将得到的五份测试数据的预测值在横向上计算平均值或者加权平均，得到的均值作为测试集的预测值。\n",
    "\n",
    "对3个基模型分别进行五折交叉验证后，将验证集预测值拼接和对测试集均值计算后，将验证集拼接而成的预测值作为新特征，使用新特征训练次级模型，使用训练好的次级模型对测试集的均值进行计算，而不是对源测试集进行预测。\n",
    "最后得到的预测结果，即为对数据集的预测"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码文件和图片文件在同一个目录下\n",
    "![title](./image/stackings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是对stacking融合的代码实现\n",
    "\n",
    "功能流程：\n",
    "- 对鸢尾花数据进行3分类\n",
    "- 将其切分为训练集和测试集【因为数据比较少，所以验证集和测试集相同，代码中统称为为测试集】\n",
    "- xgb和lgb作为基模型，分别预测概率，而不是预测标签\n",
    "- 将xgb和lgb的预测结果进行拼接和计算，训练svm次级模型，进而预测标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以python自带的鸢尾花数据集为例\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "#切分一部分数据作为测试集\n",
    "x_train,x_test,y_train,y_text = train_test_split(X, y, test_size=0.2, random_state=914)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(object):\n",
    "    \"\"\"Parent class of basic models\"\"\"\n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"return a trained model and eval metric o validation data\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, model, x_test):\n",
    "        \"\"\"return the predicted result\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_oof(self, x_train, y_train, x_test, n_folds = 5):\n",
    "        \"\"\"K-fold stacking\"\"\"\n",
    "        num_train, num_test = x_train.shape[0], x_test.shape[0]\n",
    "        oof_train = np.zeros((num_train,3)) \n",
    "        oof_test = []\n",
    "        oof_test_all_fold = np.zeros((num_test, n_folds))\n",
    "        aucs = []\n",
    "        KF = KFold(n_splits = n_folds, random_state=2017)\n",
    "        for i, (train_index, val_index) in enumerate(KF.split(x_train)):\n",
    "            print('{0} fold, train {1}, val {2}'.format(i, len(train_index), len(val_index)))\n",
    "            x_tra, y_tra = x_train[train_index], y_train[train_index]\n",
    "            x_val, y_val = x_train[val_index], y_train[val_index]\n",
    "            model, auc = self.train(x_tra, y_tra, x_val, y_val)\n",
    "            oof_train[val_index] = self.predict(model, x_val)\n",
    "            oof_test.append(self.predict(model, x_test))\n",
    "        oof_test = np.mean(np.hstack(oof_test), axis=1)\n",
    "#         print('all aucs {0}, average {1}'.format(aucs, np.mean(aucs)))\n",
    "        return oof_train, oof_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "class LGBClassifier(BasicModel):\n",
    "    def __init__(self):\n",
    "        self.num_boost_round = 2000\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "#             'objective': 'binary',\n",
    "            'objective': 'multiclass',\n",
    "            'metric': 'multi_error',\n",
    "            'num_leaves': 80,\n",
    "            'learning_rate': 0.05,\n",
    "            # 'scale_pos_weight': 1.5,\n",
    "            'feature_fraction': 0.5,\n",
    "            'bagging_fraction': 1,\n",
    "            'bagging_freq': 5,\n",
    "            'max_bin': 300,\n",
    "            'is_unbalance': True,\n",
    "            'lambda_l2': 5.0,\n",
    "            'verbose' : -1,\n",
    "            'num_class':3\n",
    "            }\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print('train with lgb model')\n",
    "        lgbtrain = lgb.Dataset(x_train, y_train)\n",
    "        lgbval = lgb.Dataset(x_val, y_val)\n",
    "        model = lgb.train(self.params, \n",
    "                          lgbtrain,\n",
    "                          valid_sets = lgbval,\n",
    "                          verbose_eval = self.num_boost_round,\n",
    "                          num_boost_round = self.num_boost_round,\n",
    "                          early_stopping_rounds = self.early_stopping_rounds)\n",
    "        return model, model.best_score['valid_0']\n",
    "    \n",
    "    def predict(self, model, x_test):\n",
    "        print('test with lgb model')\n",
    "        a=model.predict(x_test, num_iteration=model.best_iteration)\n",
    "#         print(a.shape)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's multi_error: 0.416667\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "1 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's multi_error: 0.458333\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "2 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's multi_error: 0.583333\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "3 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's multi_error: 0.541667\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "4 fold, train 96, val 24\n",
      "train with lgb model\n",
      "Training until validation scores don't improve for 15 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's multi_error: 0.5\n",
      "test with lgb model\n",
      "test with lgb model\n",
      "(120, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "# get output of first layer models and construct as input for the second layer          \n",
    "lgb_classifier = LGBClassifier()\n",
    "lgb_oof_train, lgb_oof_test = lgb_classifier.get_oof(x_train, y_train, x_test)\n",
    "print(lgb_oof_train.shape, lgb_oof_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two models for first-layer stacking: xgb and lgb\n",
    "import xgboost as xgb\n",
    "class XGBClassifier(BasicModel):\n",
    "    def __init__(self):\n",
    "        \"\"\"set parameters\"\"\"\n",
    "        self.num_rounds=1000\n",
    "        self.early_stopping_rounds = 15\n",
    "        self.params = {\n",
    "#             'objective': 'binary:logistic',\n",
    "            'objective': 'multi:softprob',\n",
    "            'eta': 0.1,\n",
    "            'max_depth': 8,\n",
    "#             'eval_metric': 'auc',\n",
    "            'metric': 'multi_error',\n",
    "            'seed': 0,\n",
    "            'silent' : 1,\n",
    "            'num_class':3\n",
    "            \n",
    "         }\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val):\n",
    "        print('train with xgb model')\n",
    "        xgbtrain = xgb.DMatrix(x_train, y_train)\n",
    "        xgbval = xgb.DMatrix(x_val, y_val)\n",
    "        watchlist = [(xgbtrain,'train'), (xgbval, 'val')]\n",
    "        model = xgb.train(self.params, \n",
    "                          xgbtrain, \n",
    "                          self.num_rounds,\n",
    "                          watchlist,\n",
    "                          early_stopping_rounds = self.early_stopping_rounds)\n",
    "        return model, float(model.eval(xgbval).split()[1].split(':')[1])\n",
    "\n",
    "    def predict(self, model, x_test):\n",
    "        print('test with xgb model')\n",
    "        xgbtest = xgb.DMatrix(x_test)\n",
    "        return model.predict(xgbtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[2]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[3]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[4]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[5]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[6]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[7]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[8]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[9]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[10]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[11]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[12]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[13]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[14]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "[15]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "1 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "[2]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[3]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[4]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[5]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[6]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[7]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[8]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[9]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[10]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[11]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[12]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[13]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[14]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "[15]\ttrain-merror:0.01042\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "2 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:19] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[2]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[3]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[4]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[5]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[6]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[7]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[8]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[9]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[10]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[11]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[12]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[13]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[14]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[15]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "3 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:20] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.03125\tval-merror:0.08333\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "[2]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "[3]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "[4]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[5]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[6]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[7]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[8]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[9]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[10]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[11]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[12]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[13]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[14]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[15]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "[16]\ttrain-merror:0.02083\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[1]\ttrain-merror:0.02083\tval-merror:0.04167\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "4 fold, train 96, val 24\n",
      "train with xgb model\n",
      "[18:31:20] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { metric, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 15 rounds.\n",
      "[1]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[2]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[3]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[4]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[5]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[6]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[7]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[8]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[9]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[10]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[11]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[12]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[13]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[14]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "[15]\ttrain-merror:0.04167\tval-merror:0.08333\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-merror:0.04167\tval-merror:0.04167\n",
      "\n",
      "test with xgb model\n",
      "test with xgb model\n",
      "(120, 3) (30,)\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier()\n",
    "xgb_oof_train, xgb_oof_test = xgb_classifier.get_oof(x_train, y_train, x_test)\n",
    "print(xgb_oof_train.shape, xgb_oof_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train = [xgb_oof_train, lgb_oof_train] \n",
    "input_test = np.array([xgb_oof_test, lgb_oof_test]).T\n",
    "input_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 6) (30, 2)\n"
     ]
    }
   ],
   "source": [
    "stacked_train = np.concatenate(input_train, axis=1)\n",
    "stacked_test = input_test\n",
    "\n",
    "print(stacked_train.shape, stacked_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LR as the model of the second layer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for validation\n",
    "n = int(stacked_train.shape[0] * 0.8)\n",
    "x_tra, y_tra = stacked_train[:n], y_train[:n]\n",
    "x_val, y_val = stacked_train[n:], y_train[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出标签\n",
    "clf = svm.SVC(decision_function_shape='ovo',probability=False)\n",
    "clf.fit(x_tra,y_tra)\n",
    "y_pred=clf.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.01554226, 0.53502404, 0.44943371],\n",
       "       [0.01126673, 0.91413412, 0.07459915],\n",
       "       [0.01131326, 0.91407162, 0.07461512],\n",
       "       [0.01191187, 0.09403685, 0.89405128],\n",
       "       [0.01170821, 0.09416351, 0.89412828],\n",
       "       [0.93953054, 0.03317928, 0.02729018],\n",
       "       [0.93906664, 0.03347105, 0.02746231],\n",
       "       [0.01124835, 0.91409274, 0.07465891],\n",
       "       [0.93953054, 0.03317928, 0.02729018],\n",
       "       [0.01170821, 0.09416351, 0.89412828],\n",
       "       [0.01191187, 0.09403685, 0.89405128],\n",
       "       [0.01084207, 0.9026358 , 0.08652213],\n",
       "       [0.01176141, 0.094129  , 0.89410959],\n",
       "       [0.93953054, 0.03317928, 0.02729018],\n",
       "       [0.01176141, 0.094129  , 0.89410959],\n",
       "       [0.93906664, 0.03347105, 0.02746231],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.94063807, 0.03245605, 0.02690587],\n",
       "       [0.01091375, 0.90341611, 0.08567014],\n",
       "       [0.0180422 , 0.71640103, 0.26555677],\n",
       "       [0.01176141, 0.094129  , 0.89410959]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出概率\n",
    "clf = svm.SVC(decision_function_shape='ovo',probability=True)\n",
    "clf.fit(x_tra,y_tra)\n",
    "y_pred=clf.predict_proba(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1,\n",
       "        1, 2]),\n",
       " array([0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 1, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1,\n",
       "        2, 2]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import precision_score, confusion_matrix,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8., 0., 0.],\n",
       "       [0., 8., 0.],\n",
       "       [0., 1., 7.]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = compute_sample_weight(class_weight='balanced',y=y_val)\n",
    "cm =confusion_matrix(y_val, y_pred, sample_weight=sw)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 并没有用逻辑回归做鸢尾花的多分类\n",
    "# 逻辑回归\n",
    "model = LinearRegression()\n",
    "model.fit(x_tra,y_tra)\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "print(metrics.roc_auc_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stacking:\n",
    "\n",
    "1. stacking的原理介绍：https://wulc.me/2018/01/21/stacking%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/\n",
    "2. stacking项目：https://github.com/WuLC/MachineLearningAlgorithm/blob/master/python/Stacking.py\n",
    "\n",
    "svc:https://blog.csdn.net/BabyBirdToFly/article/details/72886879\n",
    "\n",
    "xgb,lgb：https://www.cnblogs.com/nxf-rabbit75/p/9748345.html\n",
    "\n",
    "xgb，lgb参数介绍：https://www.jianshu.com/p/1100e333fcab\n",
    "\n",
    "逻辑回归多分类：https://blog.csdn.net/weixin_39541558/article/details/80621692\n",
    "\n",
    "鸢尾花多分类：https://blog.csdn.net/golden1314521/article/details/46564227\n",
    "\n",
    "macro,micro评价指标：https://zhuanlan.zhihu.com/p/59862986"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stacking融合方法\n",
    "以上是手动实现stacking融合，mlxtend方法提供了一种stacking的方法，可以直接进行融合，而不用管基模型是如何预测数据，以及如何训练次级模型的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sclf = StackingClassifier([基模型],meta_classfier=次级模型)是stacking融合方法，选择好基模型和次级模型后，将sclf当作普通的模型，使用数据对其进行训练就可以。\n",
    "\n",
    "这个方法就完成了我们前面实现的stacking操作，是不是很简单，嘻嘻~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91 (+/- 0.07) [KNN]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "axis() got an unexpected keyword argument 'y_min'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-246-5fff03ad9d28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software_and_install\\install\\anaconda3.5\\lib\\site-packages\\mlxtend\\plotting\\decision_regions.py\u001b[0m in \u001b[0;36mplot_decision_regions\u001b[1;34m(X, y, clf, feature_index, filler_feature_values, filler_feature_ranges, ax, X_highlight, res, legend, hide_spines, markers, colors, scatter_kwargs, contourf_kwargs, scatter_highlight_kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m                antialiased=True)\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;31m# Scatter training data samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36maxis\u001b[1;34m(self, emit, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1811\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mymin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0memit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauto\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myauto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1813\u001b[1;33m             raise TypeError(f\"axis() got an unexpected keyword argument \"\n\u001b[0m\u001b[0;32m   1814\u001b[0m                             f\"'{next(iter(kwargs))}'\")\n\u001b[0;32m   1815\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: axis() got an unexpected keyword argument 'y_min'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "\n",
    "# 以python自带的鸢尾花数据集为例\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "\n",
    "label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, label, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    print(\"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用plot_decision_regions会报错，可能是因为我使用的mlxtend版本有点低，版本为0.16，可以用比较新的版本看看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blending\n",
    "#### 1. 单纯的Holdeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blending：将数据集分为三部分：训练集，验证集，测试集，不涉及交叉验证，使用训练集分别训练5个基模型，对验证集和测试集进行预测，将5个模型得到的预测值进行横向拼接，其中，验证集的预测值为次级模型的新特征，测试集的预测值为次级模型的测试集，然后，使用验证集形成的新特征训练次级模型Model6，用训练好的次级模型对基模型的测试集进行预测，而不是对源测试集预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./image/blending-solo.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier,ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "val auc Score: 1.000000\n",
      "Val auc Score of Blending: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# 以python自带的鸢尾花数据集为例\n",
    "data_0 = iris.data\n",
    "data = data_0[:100,:]\n",
    "\n",
    "\n",
    "target_0 = iris.target\n",
    "target = target_0[:100]\n",
    " \n",
    "#模型融合中基学习器\n",
    "clfs = [LogisticRegression(),\n",
    "        RandomForestClassifier(),\n",
    "        ExtraTreesClassifier(),\n",
    "        GradientBoostingClassifier()]\n",
    " \n",
    "#切分一部分数据作为测试集\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=914)\n",
    "\n",
    "\n",
    "#切分训练数据集为d1,d2两部分\n",
    "X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=914)\n",
    "dataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))\n",
    "dataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))\n",
    " \n",
    "for j, clf in enumerate(clfs):\n",
    "    #依次训练各个单模型\n",
    "    clf.fit(X_d1, y_d1)\n",
    "    y_submission = clf.predict_proba(X_d2)[:, 1]\n",
    "    dataset_d1[:, j] = y_submission\n",
    "    #对于测试集，直接用这k个模型的预测值作为新的特征。\n",
    "    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_d2[:, j]))\n",
    "\n",
    "\n",
    "#融合使用的模型\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(dataset_d1, y_d2)\n",
    "y_submission = clf.predict_proba(dataset_d2)[:, 1]\n",
    "print(\"Val auc Score of Blending: %f\" % (roc_auc_score(y_predict, y_submission)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. blending(holdout交叉)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的blending时引入了交叉验证的那种思想，也就是每个模型看到的Holdout集合不一样。\n",
    "和stacking流程类似，只不过这里是用到了交叉验证的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> blending融合过程\n",
    "- 在第一层上，将数据集交叉划分，在不同的模型上进行训练，即，n折交叉需要n个不同的模型，最后将模型对验证集的预测值纵向拼接，就可以得到和训练集长度一致的预测值，这个过程相当于对训练集进行了预测；\n",
    "\n",
    "1. 将数据分成5份，第一份为验证集，其它份为训练集，训练Model1，对第一份验证集进行预测\n",
    "2. 第二份为验证集，其它份为训练集，训练Model2，然后，用model2没模型对第二份验证集进行预测\n",
    "3. 第三份为验证集，其它份为训练集，训练Model3，然后，用model2没模型对第三份验证集进行预测\n",
    "4. 第四份为验证集，其它份为训练集，训练Model4，然后，用model2没模型对第四份验证集进行预测\n",
    "5. 第五份为验证集，其它份为训练集，训练Model5，然后，用model2没模型对第五份验证集进行预测\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./image/blending-cv.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型融合中用到的单个模型\n",
    "clfs = [LogisticRegression(solver='lbfgs'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    "\n",
    "# 切分一部分数据作为训练集\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=2020)\n",
    "\n",
    "dataset_blend_train = np.zeros((int(X.shape[0]/n_splits), len(clfs)))   # 每个模型的预测作为第二层的特征\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    "\n",
    "# 5折stacking\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits)\n",
    "skf = skf.split(X, y)\n",
    "\n",
    "fold = {}\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    fold[i] = (X[train], y[train], X[test], y[test])\n",
    "    \n",
    "Y_blend = []\n",
    "for j, clf in enumerate(clfs):\n",
    "    # 依次训练各个单模型\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], 5))\n",
    "    \n",
    "    # 5——fold交叉训练，使用第i个部分作为预测， 剩余的部分来训练模型， 获得其预测的输出作为第i部分的新特征。\n",
    "    X_train, y_train, X_test, y_test = fold[j]\n",
    "    clf.fit(X_train, y_train)\n",
    "    dataset_blend_train[:, j] =  clf.predict(X_test)\n",
    "    Y_blend.extend(y_test)\n",
    "    \n",
    "    # 对于测试集，直接用这k个模型的预测值作为新的特征\n",
    "    dataset_blend_test[:, j] = clf.predict(X_predict)\n",
    "        \n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "\n",
    "# T是转置，行变列，列变行，reshpae按照行，不断抽取n个数据，共70行，若是数据集有70个元素，那么，n=-1,若是有140个元素，n=2   \n",
    "dataset_blend_train = dataset_blend_train.T.reshape(70, -1)\n",
    "dataset_blend_test = np.mean(dataset_blend_test, axis=1).reshape(-1, 1)\n",
    "Y_blend = np.array(Y_blend).reshape(-1, 1)\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf.fit(dataset_blend_train, Y_blend)\n",
    "y_submission = clf.predict(dataset_blend_test)\n",
    "\n",
    "print(\"Val auc Score of Stacking: %f\" % (roc_auc_score(y_predict, y_submission)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型调参是在全量数据集上实现的，那使用stacking进行模型融合时，会不会对模型产生影响\n",
    "\n",
    "模型调参也分了训练集和测试集，在stacking时，将数据进行交叉融合就可以\n",
    "\n",
    "那会不会因为数据分布的不同，对模型产生不好的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从模型的结果， 样本集的集成和模型自身融合三个方面去整理.\n",
    "- 模型的结果方面，对于回归问题，我们可以对模型的结果进行加权融合等方式使得结果更好;对于分类问题，我们可以使用Voting的方式去得到最终的结果。 \n",
    "- 样本集的集成技术方面，我们学了Boosting和Bagging方式， 都是把多个弱分类器进行集成的技术， 但是两者是不同的。\n",
    "\n",
    "- 模型自身的融合方面， 我们学习了Stacking和Blending的原理及具体实现方法，介绍了mlxtend库里面的模型融合工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./image/融合.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比赛的融合这个问题，个人的看法来说其实涉及多个层面，也是提分和提升模型鲁棒性的一种重要方法：\n",
    "\n",
    "1. 结果层面的融合，这种是最常见的融合方法，其可行的融合方法也有很多，比如根据结果的得分进行加权融合，还可以做Log，exp处理等。在做结果融合的时候，有一个很重要的条件是模型结果的得分要比较近似，然后结果的差异要比较大，这样的结果融合往往有比较好的效果提升。\n",
    "2. 特征层面的融合，这个层面其实感觉不叫融合，准确说可以叫分割，很多时候如果我们用同种模型训练，可以把特征进行切分给不同的模型，然后在后面进行模型或者结果融合有时也能产生比较好的效果。\n",
    "3. 模型层面的融合，模型层面的融合可能就涉及模型的堆叠和设计，比如加Staking层，部分模型的结果作为特征输入等，这些就需要多实验和思考了，基于模型层面的融合最好不同模型类型要有一定的差异，用同种模型不同的参数的收益一般是比较小的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending和stacking的不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> stacking\n",
    "- stacking中由于两层使用的数据不同，所以可以避免信息泄露的问题\n",
    "- 在组队竞赛的过程中，不需要给队友分享自己的随机种子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Blending\n",
    "- 由于blending将数据分为了两部分，在最后预测时，有部分数据信息将被忽略\n",
    "- 同时，在使用第二层数据时，可能会因为第二层数据较少，产生过拟合现象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对各种模型融合进行了详细的介绍：https://blog.csdn.net/wuzhongqiang/article/details/105012739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
